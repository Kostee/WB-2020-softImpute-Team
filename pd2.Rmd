---
title: "The imputation methods big test"
author: "Jakub Kosterna, Dawid Przybyliński & Hanna Zdulska"
date: "22/04/2020"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Choosing the best-suited imputation is the daily dilemma of every data scientist. Some of they believe the crux lies in the most advanced and sophisticated, the others trust the simplest of all possible.

In this document we will try to find one objectively the finest imputation method by testing five of popular ones on eight specially selected data sets prepared for these operations.

## Imputation functions

We'll look at the two simple imputation methods with the following short markings:

1. *I1* / *modeMedian* - missing data supplemented with medians from individual columns
2. *I2* / *removeRows* - all the rows with any nonexistent values removed

```{r imputation_basic_functions}
imputation_mode_median <- function(df){
  
  Mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
  }
  
  for (i in 1L:length(df)){
    if (sum(is.na(df[,i])) > 0){
      if (mode(df[,i]) == 'character' | is.factor(df[,i])){
        to_imp <- Mode(df[,i])
        df[,i][is.na(df[,i])] <- to_imp
      }
      else{
        to_imp <- median(df[,i], na.rm = TRUE) 
        df[,i][is.na(df[,i])] <- to_imp
      }
    }
  }
  
  return(df)
}
imputation_remove_rows <- function(df){
  return (na.omit(df))
}
```

... and also three more advanced algorithms from popular libraries:

3. *I3* - *mice*
4. *I4* - *vim*
5. *I5* - *missForest*

```{r imputation_advanced_functions, message = FALSE, warning = FALSE}
library(mice)
imputation_fun_mice <- function(df){
  init <- mice(df, maxit=0) 
  meth <- init$method
  predM <- init$predictorMatrix
  imputed <- mice(df, method=meth, predictorMatrix=predM, m=5)
  completed <- complete(imputed)
  return(completed)
}
library(VIM)
imputation_fun_vim <- function(df){
  no_columns <- length(df)
  imputed <- kNN(df)
  imputed <- imputed[,1:no_columns]
  return(imputed)
}
library(missForest)
imputation_fun_missForest <- function(df){
  return(missForest(df)$ximp)
}
```

## Reading datasets

The eight data frames on which we will test these above were taken from **OpenML100 collection** and were corrected specifically for this research. They can be found under the following identifiers with the following names:

* 29 - *credit-approval* PÓKI CO NIE BANGLA
* 38 - *sick*
* 56 - ...
* 188 - *eucalyptus*
* 1590 - *adult*
* 6332 - *cylinder-bands*
* 23381 - *dresses-sales*
* 40536 - *SpeedDating*
* 41278 - *okcupid-stem*

Those above have been placed in individual directories identified by id in the prepared directory.

```{r read_dataset}
DFT_REPO_DATASET_DIR = './dependencies/datasets'
read_dataset <- function(openml_id, dataset_dir = DFT_REPO_DATASET_DIR){
  
  if (!dir.exists(dataset_dir)){
    stop(paste(dataset_dir, 'does not exist' ))
  }
  
  dir <- paste(dataset_dir, paste('openml_dataset', openml_id, sep = '_'), sep ='/')
  if (!dir.exists(dir)){
    stop(paste(dir, 'does not exist' ))
  }
  
  start_dir <- getwd()
  
  # set right dir to code.R to acually work - it depends on dirlocation to create json
  setwd(dir)
  # use new env to avoid trashing globalenv
  surogate_env <- new.env(parent = .BaseNamespaceEnv)
  attach(surogate_env)
  source("code.R",surogate_env)
  
  j <- jsonlite::read_json('./dataset.json')
  j$dataset <- surogate_env$dataset
  setwd(start_dir)
  
  return(j)
}
```

In order to read all the eight datasets we will use other function, which will return a dataframe containing all the important informations about the test matrices.

```{r read_all_datasets}
read_all_datasets <- function(dataset_dir = DFT_REPO_DATASET_DIR){
  
  if (!dir.exists(dataset_dir)){
    stop(paste(dataset_dir, 'does not exist'))
  }
  
  start_dir <- getwd()
  subdirs <- dir(dataset_dir)
  ids <- sapply(subdirs, function(dir){substr(dir, 16, nchar(dir))})
  datasets_combined <- lapply(ids, function(x){read_dataset(x, dataset_dir)})
  
  datasets_combined <- t(datasets_combined)
  return(unname(t(datasets_combined)))
}
```

## Metrics functions

In order to test the same test-train splits we'll use one random seed 1357 for all datasets.

```{r train_test_split}
set.seed(1357)
train_test_split <- function(dataset, train_size){
  smp_size <- floor(train_size * nrow(dataset))
  typeof(smp_size)
  
  train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)
  
  train <- dataset[train_ind, ]
  test <- dataset[-train_ind, ]
  
  return (list(train, test))
}
```

For each machine learning model after every imputation we will get the confusion matrix and the values of four basic metrics:

* *accuracy* - $\frac{TP+TN}{TP+FP+FN+TN}$
* *precision* - $\frac{TP}{TP+FP}$
* *recall* - $\frac{TP}{TP+FN}$
* *f1* - $2*\frac{Recall * Precision}{Recall + Precision}$

```{r metrics}
get_confusion_matrix <- function(test, pred){
  return (table(Truth = test, Prediction = pred))
}
confusion_matrix_values <- function(confusion_matrix){
  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <- confusion_matrix[1,2]
  FN <- confusion_matrix[2,1]
  return (c(TP, TN, FP, FN))
}
accuracy <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return((conf_matrix[1] + conf_matrix[2]) / (conf_matrix[1] + conf_matrix[2] + conf_matrix[3] + conf_matrix[4]))
}
precision <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return(conf_matrix[1]/ (conf_matrix[1] + conf_matrix[3]))
}
recall <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return(conf_matrix[1] / (conf_matrix[1] + conf_matrix[4]))
}
f1 <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  rec <- recall(confusion_matrix)
  prec <- precision(confusion_matrix)
  return(2 * (rec * prec) / (rec + prec))
}
```

## Imputation results

```{r get_result_function, warning = FALSE, message = FALSE}
library(rpart)
get_result <- function(dataset_list, imputation_fun){
  
dataset <- dataset_list$dataset
name_of_target <- dataset_list$target
# imputation
imputation_start = Sys.time() # start to measure time
imputated_dataset <- imputation_fun(dataset) 
imputation_stop = Sys.time() # end measuring time
# train test split
train_test <- train_test_split(imputated_dataset, 0.8)
train <- as.data.table(train_test[1])
test <- as.data.table(train_test[2])
# modelling
vars <- colnames(dataset)[colnames(dataset)!=name_of_target]
my_formula <- as.formula(paste(name_of_target, paste(vars, collapse=" + "), sep=" ~ "))
modelling_start = Sys.time() # start to measure time
tree_model <- rpart(formula = my_formula, data = train,
                    method = "class", control = rpart.control(cp = 0))
y_pred <- as.data.frame(predict(tree_model, test, type = "class"))
modelling_stop = Sys.time() # end measuring time
# calculating metrics
confusion_matrix <- get_confusion_matrix(test[[name_of_target]], y_pred[,1])
accuracy_v <- accuracy(confusion_matrix)
precision_v <- precision(confusion_matrix)
recall_v <- recall(confusion_matrix)
f1_v <- f1(confusion_matrix)
classification_report <- data.frame(accuracy_v, precision_v,
                                    recall_v, f1_v)
colnames(classification_report) <- c("accuracy", "precision",
                                     "recall", "f1")
dataset_list$dataset <- NULL
# in future maybe return all dataset_list ?
# for now stick with readability
imp_method_name <- deparse(substitute(imputation_fun))
return(list( dataset_id = dataset_list$id, 
             imp_method = imp_method_name,
             confusion_matrix = confusion_matrix,
             classification_report = classification_report,
             imputation_time = imputation_stop - imputation_start,
             modelling_time = modelling_stop - modelling_start))
}
data_all <- read_all_datasets()
# imputations and targets preparation
imputations <- list(imputation_fun_vim, imputation_fun_missForest,
                  imputation_remove_rows, imputation_mode_median, imputation_fun_mice)
targets <- lapply(data_all, function(d){d$target})
```

Here are the results:

```{r warning=FALSE}
  library(knitr)
  results <- c(list(0),list(0),list(0),list(0),list(0))
  results[[1]] <- readRDS('./part_results/res1_new.rds')
  results[[2]] <- readRDS('./part_results/res2_new.rds')
  results[[3]] <- readRDS('./part_results/res3_new.rds')
  results[[4]] <- readRDS('./part_results/res4_new.rds')
  results[[5]] <- readRDS('./part_results/res5_new.rds')
  
  result_table <- function(ds_it){
    d <- data.frame(matrix(ncol = 7, nrow = 0))
    colnames(d) <- c("imputation", "accuracy", "precision", "recall", "f1", "imp_time", "mod_time")
    for (i in 1:length(imputations)){
      if (length(results[[i]][[ds_it]]) != 1){ # length equal 1 means "ERROR" was generated in partial results 
        d[i,] <- c(results[[i]][[ds_it]]$imp_method,
               round(as.numeric(results[[i]][[ds_it]]$classification_report$accuracy),3),   
               round(as.numeric(results[[i]][[ds_it]]$classification_report$precision),3),
               round(as.numeric(results[[i]][[ds_it]]$classification_report$recall),3),
               round(as.numeric(results[[i]][[ds_it]]$classification_report$f1),3),
               round(as.numeric(results[[i]][[ds_it]]$imputation_time),3),
               # @TODO konwersja jednostek, as.numeric nie rozroznia jednostek czasu
               round(as.numeric(results[[i]][[ds_it]]$modelling_time),3)) 
      }
      else{ # for error input imputation name
        if (i==3){
          d[i,1] <- "imputation_fun_mice" 
        }
        if (i==4){
          d[i,1] <- "imputation_fun_missForest" 
        }
        if (i==5){
          d[i,1] <- "imputation_fun_vim" 
        }
      }
    }
    d
  }
  
  results[[1]]
  # change i bound to number of datasets 
  for (i in 1:length(results[[1]])){ # TUTAJ BANGLA DOPÓKI NIE WEŹMIE SIĘ KTÓREGOŚ ZE ZBIORÓW 4, 10, 11, 12, 13, 14 (czyli "nowych")
    kable(result_table(i), caption = paste("Dataset id: ", results[[2]][[i]]$dataset_id, sep = ""))
  }
  
  # jest taki problem że kable w pętli nie wypluwa wyniku, chyba że damy print(kable()) wtedy jednak brzydko wygląda
  
  kable(result_table(1), caption = paste("Dataset id: ", results[[2]][[1]]$dataset_id, sep = ""))
  kable(result_table(2), caption = paste("Dataset id: ", results[[2]][[2]]$dataset_id, sep = ""))
  kable(result_table(3), caption = paste("Dataset id: ", results[[2]][[3]]$dataset_id, sep = ""))
  kable(result_table(4), caption = paste("Dataset id: ", results[[2]][[4]]$dataset_id, sep = ""))
  kable(result_table(5), caption = paste("Dataset id: ", results[[2]][[5]]$dataset_id, sep = ""))
  kable(result_table(6), caption = paste("Dataset id: ", results[[2]][[6]]$dataset_id, sep = ""))
  kable(result_table(7), caption = paste("Dataset id: ", results[[2]][[7]]$dataset_id, sep = ""))
  kable(result_table(8), caption = paste("Dataset id: ", results[[2]][[8]]$dataset_id, sep = ""))
  kable(result_table(9), caption = paste("Dataset id: ", results[[2]][[9]]$dataset_id, sep = ""))
  kable(result_table(10), caption = paste("Dataset id: ", results[[2]][[10]]$dataset_id, sep = ""))
  kable(result_table(11), caption = paste("Dataset id: ", results[[2]][[11]]$dataset_id, sep = ""))
  kable(result_table(12), caption = paste("Dataset id: ", results[[2]][[12]]$dataset_id, sep = ""))
  kable(result_table(13), caption = paste("Dataset id: ", results[[2]][[13]]$dataset_id, sep = ""))
  kable(result_table(14), caption = paste("Dataset id: ", results[[2]][[14]]$dataset_id, sep = ""))
```

## Comparing imputation times

TODO:
1. weź w ramkę danych czasy imputacji po zbiorach (x - id zbioru, y - czas imputacji)
2. Zrób z tego ggplota z wykresem liniowym (jakiś ładny theme, title, itp. of course)
3. Wysnuj wnioski o najszybszych i najwolniejszych imputacjach


facet?
x - id zbioru, y - czas imputacji, z - procent braków
kategoryczny, ciągły, ciągły

facet?
x - id zbioru, y - czas imputacji, z - wielkość zbioru
kategoryczny, ciągły, ciągły


## Comparing modelling times

TODO: to co wyżej, ale na czasach modelowania. Pewnie wniosek, że czas modelowania mega podobny

## Best measures

TODO:
1. weź w CZTERY ramki danych cztery miary po zbiorach (x - id zbioru, y - wynik miary)
2. Z pomocą grid_arrange() połącz cztery ggploty z wykresami liniowymi
3. Wysnuj wnioski o najlepszych i najgorszych miarach na podstawie obrazka

4. Wylicz średnie i wariancje dla wszystkich analizowanych zbiorów dla czterech miar
5. Zrób prostego ggplota średnich, a imputacji
6. Zrób prostego ggplota wariancji, a imputacji
7. Wysnuj wnioski z powyższych dwóch wykresów na podstawie wyliczonych prostych miar

## imputation time and its results

1. Wykres po zbiorach: średnich czasów imputacji a średnich wyników miar.
2. Wnioski z tego wykresu. Czy są imputacji i lepsze i szybsze? Czy są i wolniejsze i gorsze? PYTANIE: jaki wykres, w jakiej formie?

## Conclusion

TODO: 1. Podsumowanie i wnioski


OPCJONALNIE: porównanie wielkości zbiorów a czasów imputacji